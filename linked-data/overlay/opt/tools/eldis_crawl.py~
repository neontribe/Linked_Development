"""
copyright 2013 neontribe ltd neil@neontribe.co.uk
based on a script provided by Tim Davies

takes three arguments, the first the data_url to query 
the second is a loop number
the third is an output directory

eg python eldis_crawl.py http://www.getdata.com 0 /home/neil/getdata
"""


#Crawler for Bridge / ELDIS
#Currently works in one big batch - but may be better to rework so that it outputs a file for every 100 records, and then to merge and upload those later...

import getopt
import json
import os
import sys
import urllib2

from rdflib.graph import Graph
from rdflib.namespace import Namespace, NamespaceManager
from rdflib.term import Literal, URIRef
from urlparse import urlparse, urlunparse


class Eldis(object):

    database = 'eldis'
    token = 'c2ee7827-83de-4c99-b336-dbe73d340874'
    
    RDF = Namespace("http://www.w3.org/1999/02/22-rdf-syntax-ns#")
    RDFS = Namespace("http://www.w3.org/2000/01/rdf-schema#")
    OWL = Namespace("http://www.w3.org/2002/07/owl#")
    DC = Namespace("http://purl.org/dc/elements/1.1/")
    DCTERMS = Namespace("http://purl.org/dc/terms/")
    DBPEDIA = Namespace("http://dbpedia.org/ontology/")
    DBPROP = Namespace("http://dbpedia.org/property/")
    DBRES = Namespace("http://dbpedia.org/resource/")
    FAO = Namespace("http://www.fao.org/countryprofiles/geoinfo/geopolitical/resource/")
    IATI = Namespace("http://tools.aidinfolabs.org/linked-iati/def/iati-1.01#")
    FOAF = Namespace("http://xmlns.com/foaf/0.1/")
    SKOS = Namespace("http://www.w3.org/2004/02/skos/core#")
    BIBO = Namespace("http://purl.org/ontology/bibo/")
    BASE = Namespace("http://linked-development.org/"+database +"/")

    
    def __init__(self, out_dir='/home/eldis/', data_url=None, loop=1):
        self.graph = Graph()
        self.graph.namespace_manager.bind('owl', self.OWL, override=False)
        self.graph.namespace_manager.bind('dc', self.DC, override=False)
        self.graph.namespace_manager.bind('dcterms', self.DCTERMS, override=False)
        self.graph.namespace_manager.bind('dbpedia', self.DBPEDIA, override=False)
        self.graph.namespace_manager.bind('dbprop', self.DBPROP, override=False)
        self.graph.namespace_manager.bind('dbres', self.DBRES, override=False)
        self.graph.namespace_manager.bind('fao', self.FAO, override=False)
        self.graph.namespace_manager.bind('iati', self.IATI, override=False)
        self.graph.namespace_manager.bind('foaf', self.FOAF, override=False)
        self.graph.namespace_manager.bind('skos', self.SKOS, override=False)
        self.graph.namespace_manager.bind('bibo', self.BIBO, override=False)
        self.graph.namespace_manager.bind('base', self.BASE, override=False)
        self.out_dir = out_dir
        if data_url:
            self.data_url = data_url
        else:
            contfile = open(outdir + 'nexturl', 'r')
            data_url = contfile.readline()
            contfile.close()
        self.loop = loop
        
    def dbpedia_url(self, string):
        string = string[0].upper() + string[1:].lower()
        string = string.replace(" ","_")
        return string
    
    def fetch_data(self, data_url):
        req = urllib2.Request(data_url)
        req.add_header('Accept', 'application/json')
        req.add_header('Token-Guid', self.token)
        try:
            resp = urllib2.urlopen(req)
            content = json.loads(resp.read())
        except Exception as inst:
            print inst
            print "ERROR fetching" + data_url
        return content
    
    # Replace [ and ] if they occur in the path, query or fragment
    def fix_iri(self, url):
        urlobj = urlparse(url)
        path = urlobj.path.replace('[',"%5B").replace(']',"%5D")
        query = urlobj.query.replace('[',"%5B").replace(']',"%5D")
        fragment = urlobj.fragment.replace('[',"%5B").replace(']',"%5D")
        return url.replace(urlobj.path,path).replace(urlobj.query,query).replace(urlobj.fragment,fragment)
    
    def build_graph(self):
        """
        ok this is fun.
        using rdflib 3.4, none of these commands
        self.graph.remove((None,None,None))
        self.grpah.close()
        self.graph = None
        self.graph = Graph()
        
        free the ram used, they all make empty graphs, so if we iterate
        over reading in files to graphs our memory usage spirals. on 2013/04/12 
        the memory usage for  http://api.ids.ac.uk/openapi/"+eldis.database+"/get_all/documents/full
        in 1000 record chunks was 1.5G, if that memory is not available then the process is KILLED
        
        I cannot find a way to free this from inside python have looked at gc module, I suspect this
        may lie in some underlieing code.
        
        the current fix will to to write out to a file either a follow up url or 'finished', 
        and take this as the input, and run a loop from outside this code to spawn a series
        of python processes so the memory is always freed when the process ends.
        
        """
        
        print "Reading "+self.data_url
        content = self.fetch_data(self.data_url)
        try:
            for document in content['results']:
                uri = self.BASE['output/' + document['object_id'] +'/']
                self.graph.add((uri,self.DCTERMS['title'],Literal(document['title'])))
                try:
                    self.graph.add((uri,self.DCTERMS['abstract'],Literal(document['description'])))
                except:
                    pass
                self.graph.add((uri,self.DCTERMS['type'],self.DCTERMS['Text']))
                self.graph.add((uri,self.RDF['type'],self.BIBO['Article']))
                self.graph.add((uri,self.DCTERMS['identifier'],URIRef(document['metadata_url'])))
                self.graph.add((uri,self.DCTERMS['date'],Literal(document['publication_date'].replace(' ','T'))))
                self.graph.add((uri,self.DCTERMS['language'],Literal(document['language_name'])))
                self.graph.add((uri,self.RDFS['seeAlso'],URIRef(document['website_url'].replace('display&','display?'))))
        
                for author in document['author']:
                    self.graph.add((uri,self.DCTERMS['creator'],Literal(author)))
    
                try:
                    for publisher in document['publisher_array']['Publisher']:
                        puburi = self.BASE['organisation/' + publisher['object_id'] +'/']
                        self.graph.add((uri,self.DCTERMS['publisher'],puburi))
                        self.graph.add((puburi,self.DCTERMS['title'],Literal(publisher['object_name'])))
                        self.graph.add((puburi,self.FOAF['name'],Literal(publisher['object_name'])))
                        self.graph.add((puburi,self.RDF['type'],self.DBPEDIA['Organisation']))
                        self.graph.add((puburi,self.RDF['type'],self.FAO['organization']))
                        self.graph.add((puburi,self.RDF['type'],self.FOAF['organization']))
                        # We could follow this URL to get more information on the organisation...
                        self.graph.add((puburi,self.RDFS['seeAlso'],publisher['metadata_url']))         
                except:
                    #This could be improved. Bridge and Eldis appear to differ on publisher values
                    self.graph.add((uri,self.DCTERMS['publisher'],Literal(document['publisher']))) 
    
                #ELDIS / BRIDGE Regions do not map onto FAO regions effectively. We could model containments in future...
                try:
                    for region in document['category_region_array']['Region']:
                        regionuri = self.BASE['regions/' + region['object_id'] +'/']
                        self.graph.add((uri,self.DCTERMS['coverage'],regionuri))
                        self.graph.add((regionuri,self.RDFS['label'],Literal(region['object_name'])))            
                except:
                    pass
    
    
                try:
                    for country in document['country_focus_array']['Country']:
                        countryuri = self.BASE['countries/' + country['object_id'] +'/']
                        self.graph.add((uri,self.DCTERMS['coverage'],countryuri))
                        self.graph.add((countryuri,self.RDFS['label'],Literal(country['object_name']))) 
                        self.graph.add((countryuri,self.FAO['codeISO2'],Literal(country['iso_two_letter_code']))) 
                        self.graph.add((countryuri,self.RDFS['seeAlso'],URIRef(country['metadata_url'])))
                        self.graph.add((countryuri,self.OWL['sameAs'],self.DBRES[country['object_name']]))
                        self.graph.add((countryuri,self.OWL['sameAs'],self.FAO[country['object_name']]))
                except:
                    pass
        
    
                try:
                    for category in document['category_theme_array']['theme']:
                        themeuri = self.BASE['themes/' + category['object_id'] +'/']
                        self.graph.add((uri,self.DCTERMS['subject'],themeuri))
                        self.graph.add((themeuri,self.RDF['type'],self.SKOS['Concept']))
                        self.graph.add((themeuri,self.RDFS['label'],Literal(category['object_name']))) 
                        self.graph.add((themeuri,self.RDFS['seeAlso'],URIRef(category['metadata_url'])))
                        self.graph.add((themeuri,self.OWL['sameAs'],self.dbpedia_url(self.DBRES[category['object_name']])))
                except:
                    pass
    
                try:
                    for document_url in document['urls']:
                        self.graph.add((uri,self.BIBO['uri'],fix_iri(document_url)))
                except:
                    pass
            rdf = open(self.out_dir + 'rdf/'+self.database+'-'+str(self.loop)+'.rdf','w')
            rdf.write(self.graph.serialize())
            rdf.close()

            self.graph.remove((None,None,None))
            
            contfile = open(self.out_dir + 'nexturl', 'w')
            try:
                if(content['metadata']['next_page']):
                    contfile.write(content['metadata']['next_page'])
                    print str(int(content['metadata']['total_results']) - int(content['metadata']['start_offset'])) + " records remaining"
                    #self.build_graph(content['metadata']['next_page'],n+1)
                else:
                    print "Build complete"
            except:
                contfile.write("No more pages")
                print "No more pages"
            contfile.close()
        except Exception as inst:
            print inst
            print "Failed to read "+ self.data_url

def usage(e=None):
    if e:
        print >> sys.stderr, "Error:", e
    print >> sys.stderr, "Syntax: %s" % sys.argv[0]
    print >> sys.stderr, __doc__
    sys.exit(1)

def main():
    try:
        opts, args = getopt.gnu_getopt(sys.argv[1:], "", [])
    except getopt.GetoptError, e:
        usage(e)
    data_url = "http://api.ids.ac.uk/openapi/"+'eldis'+"/get_all/documents/full?num_results=1000"
    loop = 0
    out_dir='/home/neil/eldis/'
        
    if len(args) > 0:
        data_url = args[0]
    if len(args) > 1:
        loop = args[1]
    if len(args) == 3:
        out_dir = args[2]
    if not out_dir[-1:] == os.sep:
        out_dir = out_dir + os.sep
    eldis = Eldis(out_dir,
                  data_url,
                  loop)
    eldis.build_graph()

if __name__ == "__main__":
    main()
